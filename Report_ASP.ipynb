{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Report.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "LmLTLZhu1lDz",
        "O9zwywuJ7aKz"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvXYHLkshJ8i",
        "colab_type": "text"
      },
      "source": [
        "#Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RNrj_3j1zsy",
        "colab_type": "text"
      },
      "source": [
        "This project aims at evaluating the importance of transformations such as reverberation applied to the input data of a machine learning-based speech recognition system. Through the use of the *pyroomacoustics* library, we have created a toolbox to test speech recognition performances against reverberated datasets. Separately, we also implemented a real time keyword classifier based on a pretrained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LmLTLZhu1lDz"
      },
      "source": [
        "# Speech recognition in a reverberated environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7ndvihjhJ8k",
        "colab_type": "text"
      },
      "source": [
        "## Review of existing techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBWwg9q_hJ8l",
        "colab_type": "text"
      },
      "source": [
        "Our project’s starting point was  the tutorial [1], which explains how to build a basic speech recognition network in order to differentiate the ten words ”yes”, ”no”, ”up”, ”down”, ”left”,”right”,  ”on”,  ”off”,  ”stop”,  and  ”go”.  The  tutorial  uses  the speech commands dataset, collected by google, and described in [2]. The tutorial’s algorithm is trained using the architecture described in [3], using convolutional neural networks instead of  deep  neural  networks  which  are  in  use  for  keyword  classification in current applications. This technique is very close to  building  a  network  for  image  classification,  as  the  sound is converted into a spectrogram, and this spectrogram is used as input of the network. The spectrogram represents all frequency components as a function of time, and it can be represented in 2 dimensions. The tutorial [1] uses a standard spectrogram, but actually, the human ear being more sensitive to certain frequencies than others, a better representation  for  speech  recognition would be the Mel-Frequency Cepstral Coefficients.\n",
        "Other approaches to build neural networks working with audio include  recurrent  networks  or  dilated  (atrous)  convolutions.  \n",
        "Pyroomacoustics  is  a  python  package  for  audio  signal  processing for indoor applications. The main function we use in the  scope  of  our  project  is  the  simulation  of  a  room  and  its impulse response. We augment the dataset [2] by convolving every input sound with different impulse responses, to obtain more data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcyIWu0ihJ8m",
        "colab_type": "text"
      },
      "source": [
        "## Algorithm description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iO8jx0H6hJ8m",
        "colab_type": "text"
      },
      "source": [
        "When developing tools for the project, all the code has been written in object-oriented paradigm in order to create an easily adaptable toolbox to test several configurations for the speech recognition framework. This toolbox was based on the already existing \\textit{pyroomacoustics} Python library to work in conjunction with the Google Speech Commands Dataset. The speech recognition system is a machine learning-based speech recognition system from the TensorFlow library.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qro-VteihJ8n",
        "colab_type": "text"
      },
      "source": [
        "**Choice of the dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIvWw-FmhJ8o",
        "colab_type": "text"
      },
      "source": [
        "To train the speech recognition system, we have to define a speech dataset. We chose the Google Speech Commands Dataset, which consists of 105,000 .wav audio files of people saying thirty different words. This dataset is very interesting both for its size and diversity.\n",
        "\n",
        "\n",
        "As it is a collection of user-contributed audio files, words are pronounced by several different speakers and in various environments. This is interesting to note that the recording conditions differ according to the users that recorded it, and that they represent various living rooms in which a speech recognition system could be used.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ER0ftep3hJ8p",
        "colab_type": "text"
      },
      "source": [
        "**Dataset transformation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQkLA2OyhJ8p",
        "colab_type": "text"
      },
      "source": [
        "We load the dataset thanks to the \\textit{pyroomacoustics} Google Speech dataset wrapper. It allows us to get a dataset object from the directory where we stored the dataset. The dataset object is a convenient way to manipulate the whole dataset. Once it is loaded, we can access each sample and its corresponding label, as well as the path where it is stored.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HpFEa04hJ8q",
        "colab_type": "text"
      },
      "source": [
        "The dataset transformation consists of two scripts : \n",
        "- the 'speech_dataset_load.py' script that loads all the samples and labels into a dataset object\n",
        "- the 'reverb_data.py' script that creates a copy of the whole dataset and that applies a reverberation algorithm to each individual sample but maintains the original label of the sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgBleo6YhJ8r",
        "colab_type": "text"
      },
      "source": [
        "At the beginning of the 'speech_dataset_load' script we load the Speech Dataset locally. There is no trick on this part of the project since it consists in a simple utilisation of the pyroomacoustics library. Let us note that the dataset also contains noise samples that are mixed with the speech samples during the training process in order to obtain a more robust speech recognition model in the end.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMQW6v_bW2YI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_speech_dataset(subset, playsound=False, noplot=True):\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='Example of using the GoogleSpeechCommands wrapper')\n",
        "    parser.add_argument('--noplot', action='store_true',\n",
        "            help='Do not display any plot')\n",
        "    parser.add_argument('--playsound', action='store_true',\n",
        "            help='Play one example sentence')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # create a subset of the Google Speech Commands dataset that contains 10 of each word and the noise samples\n",
        "    data_path = '/tensorflow-master/tensorflow/examples/speech_commands/data'\n",
        "    dataset = pra.datasets.GoogleSpeechCommands(basedir= os.getcwd() + '/' + data_path, download=False, subset=subset)\n",
        "\n",
        "    # print dataset info and first 10 entries\n",
        "    print(dataset)\n",
        "    print()\n",
        "    dataset.head(n=10)\n",
        "\n",
        "    # separate the noise and the speech samples\n",
        "    print()\n",
        "    noise_samps = dataset.filter(speech=0)\n",
        "    print(\"Number of noise samples : %d\" % len(noise_samps))\n",
        "    speech_samps = dataset.filter(speech=1)\n",
        "    print(\"Number of speech samples : %d\" % len(speech_samps))\n",
        "\n",
        "    # print info of first speech sample\n",
        "    print()\n",
        "    print(speech_samps[0])\n",
        "\n",
        "    # list sounds in our dataset and number of occurences\n",
        "    print()\n",
        "    print(\"All sounds in the dataset:\")\n",
        "    print(dataset.classes)\n",
        "\n",
        "    # filter by specific word\n",
        "    selected_word = dataset.classes[1]\n",
        "    matches = speech_samps.filter(word=selected_word)\n",
        "    print()\n",
        "    print(\"Number of '%s' samples : %d\" % (selected_word, len(matches)))\n",
        "\n",
        "    # play sound\n",
        "    if playsound:\n",
        "        matches[0].play()\n",
        "\n",
        "    # show the spectrogram\n",
        "    if not noplot:\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        plt.figure()\n",
        "        matches[0].plot()\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHyBgzdChJ8r",
        "colab_type": "text"
      },
      "source": [
        "For the 'reverb_data.py' script, we first load the whole dataset by calling the *load_dataset* function. Then, we create two entities with the function *separate_dataset* : one for the noise samples that will just be copied, and one for the speech samples that will be transformed with reverberation. In parallel, the data directory structure is copied in order to keep the label information for each speech sample in *init_new_folder*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oifU3IbJW77P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(self, subset=10):\n",
        "    \"\"\"\n",
        "    Calls the load_speech_dataset function that creates a Dataset object from the GSD.\n",
        "    \"\"\"\n",
        "    print('Loading the Google Speech Dataset.')\n",
        "    print()\n",
        "    self.raw_dataset = load_speech_dataset(subset)\n",
        "    print('Google Speech Dataset successfully loaded.')\n",
        "    print()\n",
        "\n",
        "def separate_dataset(self):\n",
        "    \"\"\"\n",
        "    Separates the dataset into speech/noise.\n",
        "    \"\"\"\n",
        "    print('Separate dataset between speech and noise :')\n",
        "    self.raw_only_speech = self.raw_dataset.filter(speech=True)\n",
        "    self.raw_only_noise = self.raw_dataset.filter(speech=False)\n",
        "    print('Successfully separated the dataset.')\n",
        "\n",
        "def init_new_folder(self, new_name):\n",
        "    \"\"\"\n",
        "    This method creates and initiates the subfolders containing each wav file.\n",
        "    \"\"\"\n",
        "    self.new_folder_name = new_name\n",
        "    self.new_path = re.sub('data', self.new_folder_name, self.raw_dataset.basedir)\n",
        "\n",
        "    print('The new folder is located at : {0}'.format(self.new_path))\n",
        "    if not os.path.isdir(self.new_path):\n",
        "        os.makedirs(self.new_path)\n",
        "\n",
        "    for subdir in self.raw_dataset.subdirs:\n",
        "        extracted_subdir = re.split('/', subdir)[-2]\n",
        "        # We do not want to apply reverb on the background noise folder,\n",
        "        # so we exclude this folder during this step\n",
        "        if extracted_subdir != '_background_noise_':\n",
        "            if not os.path.exists(self.new_path + '/' + extracted_subdir):\n",
        "                os.makedirs(self.new_path + '/' + extracted_subdir)\n",
        "\n",
        "    print('New Folder initiated.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_w92xdTXFIn",
        "colab_type": "text"
      },
      "source": [
        "After this initial phase, we simply apply reverberation to every sample in the *dataset* object. For each sample, we retrieve the audio file, create a reverberation model from the class *ReverbModel* and apply the reverberation transformation to this sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYFXc5XKXG0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_reverb_model(self):\n",
        "    \"\"\"\n",
        "    Inits the reverberation model to be applied\n",
        "    \"\"\"\n",
        "    self.reverb_model = reverb_model.ReverbModel()\n",
        "    self.reverb_model.reverb_model1()\n",
        "\n",
        "def apply_reverb(self):\n",
        "    \"\"\"\n",
        "    Applies reverberation to all the samples in the dataset\n",
        "    \"\"\"\n",
        "    print('Applying reverberation algorithm to the whole dataset : ')\n",
        "\n",
        "    for idx_sample, sample in enumerate(self.raw_only_speech):\n",
        "        self.init_reverb_model()\n",
        "        audio_sample_no_rev = sample.data\n",
        "        output_path = re.sub('data', self.new_folder_name, sample.meta.file_loc)\n",
        "        self.reverb_model.transform_audio(audio_sample_no_rev, output_path)\n",
        "\n",
        "        if (idx_sample % 500 == 0) and (idx_sample > 0):\n",
        "            print('Now treating sample {0}.'.format(idx_sample))\n",
        "\n",
        "    print('Reverberation applied to the whole speech dataset.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4MhfxAAXJEj",
        "colab_type": "text"
      },
      "source": [
        "Below is a part of the *ReverbModel* class :\n",
        "- *reverb_model1* is one of the methods creating a reverberation model\n",
        "- transform_audio simply takes the reverberation model, the input audio as a source, and the signal captated from the microphone as the output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEyCuTKUXLqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reverb_model1(self, absorption = 0.2, max_order = 15):\n",
        "    self.room_dimensions = [6, 5, 7]\n",
        "    self.absorption = absorption\n",
        "    self.max_order = max_order\n",
        "    self.fs = 16000\n",
        "\n",
        "    self.shoebox = pra.ShoeBox(\n",
        "        self.room_dimensions,\n",
        "        absorption=self.absorption,\n",
        "        fs=self.fs,\n",
        "        max_order=self.max_order,\n",
        "    )\n",
        "\n",
        "    self.source_position = [2, 3.1, 2]\n",
        "    self.mic = pra.MicrophoneArray(np.array([[2, 1.5, 2]]).T, self.shoebox.fs)   \n",
        "\n",
        "def transform_audio(self, audio_sample, output_path):\n",
        "    \"\"\"\n",
        "    Applies reverberation to the audio_sample.\n",
        "    The source in the virtual room plays the audio_sample signal.\n",
        "    The output is the result of the captation of the microphone in the virtual room.\n",
        "    :param audio_sample: data signal we want to apply reverb on\n",
        "    :param output_path: path where we want to store the output of the reverberation algorithm\n",
        "    \"\"\"\n",
        "    self.shoebox.add_source(self.source_position, signal=audio_sample)\n",
        "    self.shoebox.add_microphone_array(self.mic)\n",
        "\n",
        "    self.shoebox.simulate()\n",
        "    self.shoebox.mic_array.to_wav(output_path, norm=True, bitdepth=np.int16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmZqJipwXO_s",
        "colab_type": "text"
      },
      "source": [
        "For the reverberation model, we have taken simple models : rectangular rooms (shoeboxes) with one source and one microphone. As stated above, the source is playing the input audio, and the output audio is the signal captated from the microphone.\n",
        "\n",
        "The taken parameters where typical of listening conditions in a big room : \n",
        "- absorption coefficient between 0.1 and 0.2\n",
        "- dimensions around 5 meters\n",
        "- source and microphones placed in the room, not to close to the walls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2-laGqhXStq",
        "colab_type": "text"
      },
      "source": [
        "In total, we have 4 reverberation models ranging to the least reverberating one to the most reverberating one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJyfEhueXSrG",
        "colab_type": "text"
      },
      "source": [
        "At the end of the process, we write the output file in the new folder containing all the reverberated samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQAD6fNhXWuQ",
        "colab_type": "text"
      },
      "source": [
        "**Mixed Reverberation model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLhYzXnbXW1n",
        "colab_type": "text"
      },
      "source": [
        "The above part details the process of transforming the original dataset into a new dataset where every sample is reverberated. For each sample we took the same reverberation model.\n",
        "One way to explore improvements to the original speech recognition model is to change the reverberation parameters for each sample. Instead of having one reverberation model for each sample, we have an ensemble of reverberation parameters from which we choose one for each sample randomly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7J1iVJ_Xi7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_random_reverb_model(self):\n",
        "    \"\"\"\n",
        "    Init the reverberation randomly among the choices in param\n",
        "    \"\"\"\n",
        "    params = random.choice(self.random_params)\n",
        "\n",
        "    self.reverb_model = reverb_model.ReverbModel()\n",
        "    self.reverb_model.reverb_model_generic(params[0], params[1], params[2])\n",
        "\n",
        "def apply_reverb_random(self, set_params):\n",
        "    \"\"\"\n",
        "    Applies reverberation to all the samples in the dataset.\n",
        "    For each sample, a random reverberation model is chosen among all the parameters given.\n",
        "    :param set_params: the set of parameters to choose from for the reverberation model.\n",
        "    \"\"\"\n",
        "    print('Applying reverberation to the whole dataset with randomized parameters.')\n",
        "    self.random_params = set_params\n",
        "\n",
        "    for idx_sample, sample in enumerate(self.raw_only_speech):\n",
        "        self.init_random_reverb_model()\n",
        "        audio_sample_no_rev = sample.data\n",
        "        output_path = re.sub('data', self.new_folder_name, sample.meta.file_loc)\n",
        "        self.reverb_model.transform_audio(audio_sample_no_rev, output_path)\n",
        "\n",
        "        if (idx_sample % 500 == 0) and (idx_sample > 0):\n",
        "            print('Now treating sample {0}.'.format(idx_sample))\n",
        "\n",
        "    print('Reverberation applied to the whole speech dataset.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjWew0Wv_oEZ",
        "colab_type": "text"
      },
      "source": [
        "This acted as a fifth reverberated dataset, being a mix of multiple reverberation models.\n",
        "\n",
        "In total, we had :\n",
        "- the dry model (the original Google Speech Dataset)\n",
        "- 4 reverberated models\n",
        "- the mixed parameters model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSX5N6YJYc6p",
        "colab_type": "text"
      },
      "source": [
        "**Training the model with reverberated dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsFZgkN46lTZ",
        "colab_type": "text"
      },
      "source": [
        "The training part was performed using *TensorFlow* implementation. The part we changed is the input data we give to the algorithm to process. The command below was used to train the *TensorFlow* model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc8QMU-F8psi",
        "colab_type": "text"
      },
      "source": [
        "We used the script *train.py* from the Audio Recognition *TensorFlow* GitHub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NHkPmMX9IyX",
        "colab_type": "text"
      },
      "source": [
        "Let us note that we had to use clusters provided by EPFL to run this part : first trainings took more thant 15 hours on our laptops, as it consists of 18 000 training steps, which corresponds to a huge training process. By using cluster computers, it took less than 4 hours and we were able to run multiple training instances simultaneously."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJSFLTH2hJ8t",
        "colab_type": "text"
      },
      "source": [
        "## Testing the performances of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkY_ccgaXq6c",
        "colab_type": "text"
      },
      "source": [
        "Here we detail the way we test the performances of the trained models in the *test_model* script.\n",
        "\n",
        "Once we have trained the speech recognition model with our data, be it the original one, or a reverberated one, we obtain a .pb graph representing our final trained model. With this graph we are able to test the model on validation files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RD8coWw3CNfO",
        "colab_type": "text"
      },
      "source": [
        "Validation files are files that we did not use during training, and we kept only to test the final model in the end. For each one, we run the script *label_wav* (based on the *TensorFlow* project implementation) that outputs the scores for each label between 0 and 1.\n",
        "\n",
        "Given a sample and a graph, we thus obtain a score giving the prediction that the model gives on each label. We then average the prediction accuracy for each true label on a set of validation files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9jH4YP7SyjB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def label_wav(self, wav, labels, graph, input_name, output_name, how_many_labels):\n",
        "        \"\"\"Loads the model and labels, and runs the inference to print predictions.\"\"\"\n",
        "        if not wav or not tf.io.gfile.exists(wav):\n",
        "            tf.compat.v1.logging.fatal('Audio file does not exist %s', wav)\n",
        "\n",
        "        if not labels or not tf.io.gfile.exists(labels):\n",
        "            tf.compat.v1.logging.fatal('Labels file does not exist %s', labels)\n",
        "\n",
        "        if not graph or not tf.io.gfile.exists(graph):\n",
        "            tf.compat.v1.logging.fatal('Graph file does not exist %s', graph)\n",
        "\n",
        "        labels_list = self.load_labels(labels)\n",
        "\n",
        "        # load graph, which is stored in the default session\n",
        "        self.load_graph(graph)\n",
        "\n",
        "        with open(wav, 'rb') as wav_file:\n",
        "            wav_data = wav_file.read()\n",
        "\n",
        "        return self.run_graph(wav_data, labels_list, input_name, output_name, how_many_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnKYwkvMUky1",
        "colab_type": "text"
      },
      "source": [
        "We then encompass this function in a bigger one that runs the tests and stores the results. We test each model on each dataset (non-reverberated and reverberated ones)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E8w9Bg1Usay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def test_model(self, model_number):\n",
        "        self.graph_model_path = self.base_path + '/../results/frozen_graph_set' + str(model_number) + '.pb'\n",
        "        self.conv_labels_path = self.base_path + '/../results/conv_labels.txt'\n",
        "        self.scores = np.zeros(subset)\n",
        "\n",
        "        print('Starting test for model {0}'.format(model_number))\n",
        "\n",
        "        list_indices = range(len(self.list_paths))\n",
        "        sublist_indices = random.choices(list_indices,k=self.subset)\n",
        "\n",
        "\n",
        "        for id, index_sample in enumerate(sublist_indices):\n",
        "\n",
        "            if id % 50 == 0:\n",
        "                print('Currently at sample {0}'.format(id))\n",
        "\n",
        "            self.current_label = self.list_labels[index_sample]\n",
        "            self.scores[id] = self.label_wav(self.base_path + '/' + self.list_paths[index_sample],\n",
        "                                             self.conv_labels_path,\n",
        "                                                       self.graph_model_path, 'wav_data:0', 'labels_softmax:0',\n",
        "                                                       self.how_many_labels)\n",
        "        print('Test for model {0} done.'.format(model_number))\n",
        "        print()\n",
        "\n",
        "    def test_all_models(self, list_models):\n",
        "\n",
        "        self.all_scores = np.zeros(len(list_models))\n",
        "        self.full_scores = np.zeros((len(list_models), self.subset))\n",
        "        path_to_save_scores = self.base_path + '/../results/scores_' + self.data_name + '.npy'\n",
        "        path_to_save_full = self.base_path + '/../results/full_' + self.data_name + '.npy'\n",
        "        random.seed(42)\n",
        "\n",
        "        for index_model, model in enumerate(list_models):\n",
        "            self.test_model(model)\n",
        "            self.full_scores[index_model, :] = self.scores\n",
        "            self.all_scores[index_model] = np.average(self.scores)\n",
        "\n",
        "        np.save(path_to_save_scores, self.all_scores)\n",
        "        np.save(path_to_save_full, self.full_scores)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9zwywuJ7aKz",
        "colab_type": "text"
      },
      "source": [
        "#Real Time Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdaBrmke1myL",
        "colab_type": "text"
      },
      "source": [
        "The real time implementation was done using the python sounddevice library. In the original tutorial [1], there was already a C++ implementation of real time keyword classification, but it did not accept microphone inputs: it could recognise where the keywords were spoken from a long audio file, in real time. The way this algorithm worked was by applying many times the model, with a lot of different time offsets and averaging the results. This is because the words we want to recognize can start at any time, so we need to take a series of snapshots to have a chance of having an alignment that captures most of the utterance in the time window we feed into the model. If we sample at a high enough rate, then we have a good chance of capturing the word in multiple windows, so averaging the results improves the overall confidence of the prediction.  \n",
        "After getting used to the sounddevice and tensorflow libraries, the main problem we faced to implement such an algorithm in python was the fact that it is not possible to convert the sound input directly into 16 bit linear pulse code modulation format, which is the format of the input to the tensorflow graph, without changing the soundfile library. Therefore we found a way to work around this problem by saving a .wav file and reading it again, every second. Since this process considerably slows down the program, it is not possible to perform window averaging as suggested by the tutorial's C++ implementation. However, good results can already be observed with this implementation. The algorithm also plots the input on which the prediction is being made, at every second.  \n",
        "This is the main file to call to have the real-time classification, it is called real_time.py on our github. It was adapted from an example on the sounddevice library site [4], which already plotted the input in real-time. We call our classification function in update_plot, in order to classify in real_time every window that is plotted. Notice that we have to write and read a wav file in order to feed the correct format to the graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AllS8HIl6ANM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Plot the live microphone signal(s) with matplotlib, and classifies the word spoken out of 10 known keywords\n",
        "\n",
        "Matplotlib and NumPy have to be installed.\n",
        "This was adapted from an example given on sounddevice. \"\"\"\n",
        "\n",
        "\n",
        "import argparse\n",
        "import queue\n",
        "import sys\n",
        "\n",
        "from matplotlib.animation import FuncAnimation\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sounddevice as sd\n",
        "import scipy.io.wavfile as wavconv\n",
        "import soundfile as sf\n",
        "\n",
        "from label_wav_realtime import *\n",
        "\n",
        "\n",
        "def int_or_str(text):\n",
        "    \"\"\"Helper function for argument parsing.\"\"\"\n",
        "    try:\n",
        "        return int(text)\n",
        "    except ValueError:\n",
        "        return text\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(add_help=False)\n",
        "parser.add_argument(\n",
        "    '-l', '--list-devices', action='store_true',\n",
        "    help='show list of audio devices and exit')\n",
        "args, remaining = parser.parse_known_args()\n",
        "if args.list_devices:\n",
        "    print(sd.query_devices())\n",
        "    parser.exit(0)\n",
        "parser = argparse.ArgumentParser(\n",
        "    description=__doc__,\n",
        "    formatter_class=argparse.RawDescriptionHelpFormatter,\n",
        "    parents=[parser])\n",
        "parser.add_argument(\n",
        "    'channels', type=int, default=[1], nargs='*', metavar='CHANNEL',\n",
        "    help='input channels to plot (default: the first)')\n",
        "parser.add_argument(\n",
        "    '-d', '--device', type=int_or_str,\n",
        "    help='input device (numeric ID or substring)')\n",
        "parser.add_argument(\n",
        "    '-w', '--window', type=float, default=1000, metavar='DURATION',\n",
        "    help='visible time slot (default: %(default)s ms)')\n",
        "parser.add_argument(\n",
        "    '-i', '--interval', type=float, default=30,\n",
        "    help='minimum time between plot updates (default: %(default)s ms)')\n",
        "parser.add_argument(\n",
        "    '-b', '--blocksize', type=int, help='block size (in samples)')\n",
        "parser.add_argument(\n",
        "    '-r', '--samplerate', type=float, help='sampling rate of audio device')\n",
        "parser.add_argument(\n",
        "    '-n', '--downsample', type=int, default=1, metavar='N',\n",
        "    help='display every Nth sample (default: %(default)s)')\n",
        "args = parser.parse_args(remaining)\n",
        "if any(c < 1 for c in args.channels):\n",
        "    parser.error('argument CHANNEL: must be >= 1')\n",
        "mapping = [c - 1 for c in args.channels]  # Channel numbers start with 1\n",
        "q = queue.Queue()\n",
        "\n",
        "\n",
        "def audio_callback(indata, frames, time, status):\n",
        "    \"\"\"This is called (from a separate thread) for each audio block.\"\"\"\n",
        "    if status:\n",
        "        print(status, file=sys.stderr)\n",
        "    # Fancy indexing with mapping creates a (necessary!) copy:\n",
        "    q.put(indata[::args.downsample, mapping])\n",
        "\n",
        "\n",
        "def update_plot(frame):\n",
        "    \"\"\"This is called by matplotlib for each plot update.\n",
        "\n",
        "    Typically, audio callbacks happen more frequently than plot updates,\n",
        "    therefore the queue tends to contain multiple blocks of audio data.\n",
        "\n",
        "    \"\"\"\n",
        "    global i\n",
        "    i=i+1\n",
        "    global plotdata\n",
        "    while True:\n",
        "        try:\n",
        "            data = q.get_nowait()\n",
        "        except queue.Empty:\n",
        "            break\n",
        "        shift = len(data)\n",
        "        plotdata = np.roll(plotdata, -shift, axis=0)\n",
        "        plotdata[-shift:, :] = data\n",
        "\n",
        "    sf.write('wavfile.wav', plotdata, 16000, subtype='PCM_16',format='wav')\n",
        "    with open('wavfile.wav', 'rb') as wav_file:\n",
        "        bitdata = wav_file.read()\n",
        "    label = label_wav(bitdata, 'conv_labels.txt', 'first_model_graph_anechoic.pb')\n",
        "    if label[0] != 0:\n",
        "        print(label)\n",
        "\n",
        "    print(i)\n",
        "\n",
        "    for column, line in enumerate(lines):\n",
        "        line.set_ydata(plotdata[:, column])\n",
        "    return lines\n",
        "\n",
        "\n",
        "try:\n",
        "\n",
        "    args.samplerate=16000\n",
        "\n",
        "    length = int(args.window * args.samplerate / (1000 * args.downsample))\n",
        "    plotdata = np.zeros((length, len(args.channels)))\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    lines = ax.plot(plotdata)\n",
        "    if len(args.channels) > 1:\n",
        "        ax.legend(['channel {}'.format(c) for c in args.channels],\n",
        "                  loc='lower left', ncol=len(args.channels))\n",
        "    ax.axis((0, len(plotdata), -1, 1))\n",
        "    ax.set_yticks([0])\n",
        "    ax.yaxis.grid(True)\n",
        "    ax.tick_params(bottom=False, top=False, labelbottom=False,\n",
        "                   right=False, left=False, labelleft=False)\n",
        "    fig.tight_layout(pad=0)\n",
        "\n",
        "    stream = sd.InputStream(\n",
        "        device=args.device, channels=max(args.channels),\n",
        "        samplerate=args.samplerate, callback=audio_callback)\n",
        "    i=0\n",
        "    ani = FuncAnimation(fig, update_plot, interval=1000, blit=True)\n",
        "    with stream:\n",
        "        plt.show()\n",
        "except Exception as e:\n",
        "    parser.exit(type(e).__name__ + ': ' + str(e))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDg49JWy_tOq",
        "colab_type": "text"
      },
      "source": [
        "In this second file, called label_wav_realtime.py on our github, we load a pretrained tensorflow graph and make it predict on the data from the wav file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCW2YK5j-FTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_graph(filename):\n",
        "  \"\"\"Unpersists graph from file as default graph.\"\"\"\n",
        "  with tf.io.gfile.GFile(filename, 'rb') as f:\n",
        "    graph_def = tf.compat.v1.GraphDef()\n",
        "    graph_def.ParseFromString(f.read())\n",
        "    tf.import_graph_def(graph_def, name='')\n",
        "\n",
        "\n",
        "def load_labels(filename):\n",
        "  \"\"\"Read in labels, one label per line.\"\"\"\n",
        "  return [line.rstrip() for line in tf.io.gfile.GFile(filename)]\n",
        "\n",
        "\n",
        "def run_graph(wav_data, labels, input_layer_name, output_layer_name,\n",
        "              num_top_predictions):\n",
        "  \"\"\"Runs the audio data through the graph and prints predictions.\"\"\"\n",
        "  with tf.compat.v1.Session() as sess:\n",
        "    # Feed the audio data as input to the graph.\n",
        "    #   predictions  will contain a two-dimensional array, where one\n",
        "    #   dimension represents the input image count, and the other has\n",
        "    #   predictions per class\n",
        "    softmax_tensor = sess.graph.get_tensor_by_name(output_layer_name)\n",
        "    predictions, = sess.run(softmax_tensor, {input_layer_name: wav_data})\n",
        "\n",
        "    # Sort to show labels in order of confidence\n",
        "    top_k = predictions.argsort()[-num_top_predictions:][::-1]\n",
        "\n",
        "    for node_id in top_k:\n",
        "      human_string = labels[node_id]\n",
        "      score = predictions[node_id]\n",
        "      print('%s (score = %.5f)' % (human_string, score))\n",
        "    return top_k\n",
        "\n",
        "\n",
        "def label_wav(wav_data, labels, graph, input_name='wav_data:0', output_name='labels_softmax:0', how_many_labels=3):\n",
        "  \"\"\"Loads the model and labels, and runs the inference to print predictions.\"\"\"\n",
        "\n",
        "  if not labels or not tf.io.gfile.exists(labels):\n",
        "    tf.compat.v1.logging.fatal('Labels file does not exist %s', labels)\n",
        "\n",
        "  if not graph or not tf.io.gfile.exists(graph):\n",
        "    tf.compat.v1.logging.fatal('Graph file does not exist %s', graph)\n",
        "\n",
        "  labels_list = load_labels(labels)\n",
        "\n",
        "  # load graph, which is stored in the default session\n",
        "  load_graph(graph)\n",
        "\n",
        "  return run_graph(wav_data, labels_list, input_name, output_name, how_many_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GDzrqTF1AHh",
        "colab_type": "text"
      },
      "source": [
        "This real time implementation could allow us to adapt a keyword classifier to a certain room, by choosing weights trained with a room similar to the one we are in. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r121TtNchJ8t",
        "colab_type": "text"
      },
      "source": [
        "# Results and discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2thisN35c5v",
        "colab_type": "text"
      },
      "source": [
        "Scores of dry model:\n",
        "\n",
        "|        | anechoic | reverb 1 | reverb 2 | reverb 3 | reverb 4 | mixed models |\n",
        "|--------|----------|----------|----------|----------|----------|--------------|\n",
        "| scores | 72.6% | 77.2% |  70.7% | 74.9%  |  72.3% | 72.4%    |\n",
        "\n",
        "\n",
        "Scores of model 1: \n",
        "\n",
        "\n",
        "|        | anechoic | reverb 1 | reverb 2 | reverb 3 | reverb 4 | mixed models |\n",
        "|--------|----------|----------|----------|----------|----------|--------------|\n",
        "| scores | 69.1%  | 73.0% | 67.3% | 71.3% | 70.8% |  69.5%   |\n",
        "\n",
        "Scores of model 2:\n",
        "\n",
        "|        | anechoic | reverb 1 | reverb 2 | reverb 3 | reverb 4 | mixed models |\n",
        "|--------|----------|----------|----------|----------|----------|--------------|\n",
        "| scores | 63.8% | 68.7% | 61.6% | 66.8% | 67.0% | 64.7%    |\n",
        "\n",
        "Scores of model 3:\n",
        "\n",
        "|        | anechoic | reverb 1 | reverb 2 | reverb 3 | reverb 4 | mixed models |\n",
        "|--------|----------|----------|----------|----------|----------|--------------|\n",
        "| scores | 65.3% | 70.6% |64.7% | 68.1% | 65.0% |  66.8%   |\n",
        "\n",
        "Scores of model 4:\n",
        "\n",
        "\n",
        "|        | anechoic | reverb 1 | reverb 2 | reverb 3 | reverb 4 | mixed models |\n",
        "|--------|----------|----------|----------|----------|----------|--------------|\n",
        "| scores | 66.2% | 74.6%| 66.6% | 71.1% |  69.1%| 69.0%   |\n",
        "\n",
        "\n",
        "\n",
        "Scores of mixed reverb model:\n",
        "\n",
        "\n",
        "|        | anechoic | reverb 1 | reverb 2 | reverb 3 | reverb 4 | mixed models |\n",
        "|--------|----------|----------|----------|----------|----------|--------------|\n",
        "| scores | 66.1%| 70.6%| 66.2% | 69.9% | 69.5% |67.2%   |\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQul41CyPVJR",
        "colab_type": "text"
      },
      "source": [
        "The scores displayed above are the means of the scores given by the model at its output layer. The scores represent how confident the model is in its prediction. The final score is the mean of the scores over 200 samples for each dataset. For classification, the label corresponding to the highest score is the prediction.  \n",
        "Our first expectation was that we would obtain better results in the tests if the test was processed in the same way as the training data. It is interesting to see that this is not the case for all our models: it is only true for model 1. In fact, in the case of model 2, it is even the dataset that is the least well predicted.  \n",
        "One interesting result is also that the model trained on dry data is by far better than the other models, for all testing datasets. This result is a little bit disappointing as it means we have failed to improve the algorithm's performances by augmenting the dataset.  \n",
        "Another interesting result is that if you take the mean according to each dataset, the dataset made with model 1 was the easiest to predict overall, and the dataset made with model 2 was the hardest. We had tried to make the datasets more and more reverberant, so we might have expected the \n",
        "We might explain those surprising results by the fact that the original dry dataset is actually not fully dry, but includes a lot of different impulse responses, since the data comes from recordings people have made in their home. The original dataset might actually be a lot more diverse in terms of impulse responses than what we created artificially, which had 4 different impulse responses at most, and therefore gives better results. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUhU7e6NfQTA",
        "colab_type": "code",
        "outputId": "23d76a24-8517-47d0-83c5-112ecb851fd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "results= [[0.72592986, 0.77158651, 0.70728204, 0.74885913, 0.72304554, 0.72410606],\n",
        "[0.69062885, 0.7299644,  0.67326452, 0.71344629, 0.70770812, 0.69477109],\n",
        "[0.6381686,  0.6868683,  0.61563976, 0.66805035, 0.66979465, 0.6474184 ],\n",
        "[0.65324595, 0.70628374 ,0.6472106 , 0.68137294, 0.64950378 ,0.66792668],\n",
        "[0.66174808, 0.74576864, 0.66628282 ,0.71110665, 0.69089016, 0.69000403],\n",
        "[0.66073624, 0.7057416,  0.66204827, 0.6992094,  0.69452024, 0.67156678]]\n",
        "\n",
        "#means according to the dataset\n",
        "print('means of datasets', np.mean(results,axis=0))\n",
        "\n",
        "#means according to the model\n",
        "print('means of models', np.mean(results,axis=1))\n",
        "\n",
        "plt.boxplot(results)\n",
        "plt.title('Model Performances on differently reverberated datasets')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "means of datasets [0.67174293 0.72436887 0.66195467 0.70367413 0.68924375 0.68263217]\n",
            "means of models [0.73346819 0.70163054 0.65432334 0.66759061 0.69430006 0.68230376]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEICAYAAABWJCMKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2tAktM2hJ8u",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xScp3-hpXN1Z",
        "colab_type": "text"
      },
      "source": [
        "In this project, we implemented a real time keyword classifier using the sounddevice library, and we augmented the google speech commands dataset with the pyroomacoustics library to train a more robust keyword classifier. Results we obtained were not the ones we expected, but they were relevant nevertheless."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHGIC-q60rPS",
        "colab_type": "text"
      },
      "source": [
        "Bibliography:  \n",
        "[1]:https://github.com/tensorflow/docs/blob/master/site/en/r1/tutorials/sequences/audio_recognition.md\n",
        "\n",
        "[2]:Speech commands: A dataset for limited-vocabulary speech recognition, Warden,Pete 2018\n",
        "\n",
        "[3]:Convolutional neural networks for small-footprint keyword spotting, Sainath 2015  \n",
        "[4]:https://python-sounddevice.readthedocs.io/en/0.3.14/"
      ]
    }
  ]
}